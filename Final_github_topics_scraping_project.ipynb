{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1oST5STl67OjvoZH_e3fO2oTkcLu8PsXK",
      "authorship_tag": "ABX9TyOda7Cqr921OMUnx3r2Xi9y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anilsolanki2645/WebScarping/blob/main/Final_github_topics_scraping_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the steps we'll follow:\n",
        "\n",
        "- We're going to scrape https://github.com/topics\n",
        "- We'll get a list of topics. For each topic, we'll get topic title, topic page URL and topic description\n",
        "- For each topic, we'll get the top 25 repositories in the topic from the topic page\n",
        "- For each repository, we'll grab the repo name, username, stars and repo URL\n",
        "- For each topic we'll create a CSV file in the following format:\n",
        "\n",
        "```\n",
        "Repo Name,Username,Stars,Repo URL\n",
        "three.js,mrdoob,69700,https://github.com/mrdoob/three.js\n",
        "libgdx,libgdx,18300,https://github.com/libgdx/libgdx\n",
        "```"
      ],
      "metadata": {
        "id": "9ikCkzQ6_dQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scrape the list of topics from Github\n",
        "\n",
        "Explain how you'll do it.\n",
        "\n",
        "- use requests to downlaod the page\n",
        "- user BS4 to parse and extract information\n",
        "- convert to a Pandas dataframe\n",
        "\n",
        "Let's write a function to download the page."
      ],
      "metadata": {
        "id": "DLVXRLMK_pGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def get_topics_page():\n",
        "    topics_url = 'https://github.com/topics'\n",
        "    response = requests.get(topics_url)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception('Failed to load page {}'.format(topics_url))\n",
        "    doc = BeautifulSoup(response.text, 'html.parser')\n",
        "    return doc"
      ],
      "metadata": {
        "id": "RyA05XRq_Scz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = get_topics_page()"
      ],
      "metadata": {
        "id": "NrdjJ6Y0_2J9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create some helper functions to parse information from the page.\n",
        "\n",
        "To get topic titles, we can pick `p` tags with the `class` ..."
      ],
      "metadata": {
        "id": "7atfY4kkAZ1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_titles(doc):\n",
        "    selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
        "    topic_title_tags = doc.find_all('p', {'class': selection_class})\n",
        "    topic_titles = []\n",
        "    for tag in topic_title_tags:\n",
        "        topic_titles.append(tag.text)\n",
        "    return topic_titles"
      ],
      "metadata": {
        "id": "uqH02s8iADfB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`get_topic_titles` can be used to get the list of titles"
      ],
      "metadata": {
        "id": "zOORxXW0BXc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titles = get_topic_titles(doc)"
      ],
      "metadata": {
        "id": "lvyPgiuUAuW3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(titles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndeFvAPhBcB8",
        "outputId": "8f9dba2d-8425-47e6-c652-015566c6f35f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titles"
      ],
      "metadata": {
        "id": "5Ge7ZRtqBeXF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaaab0dc-67ca-455a-c11f-bf944824a923"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['3D',\n",
              " 'Ajax',\n",
              " 'Algorithm',\n",
              " 'Amp',\n",
              " 'Android',\n",
              " 'Angular',\n",
              " 'Ansible',\n",
              " 'API',\n",
              " 'Arduino',\n",
              " 'ASP.NET',\n",
              " 'Atom',\n",
              " 'Awesome Lists',\n",
              " 'Amazon Web Services',\n",
              " 'Azure',\n",
              " 'Babel',\n",
              " 'Bash',\n",
              " 'Bitcoin',\n",
              " 'Bootstrap',\n",
              " 'Bot',\n",
              " 'C',\n",
              " 'Chrome',\n",
              " 'Chrome extension',\n",
              " 'Command line interface',\n",
              " 'Clojure',\n",
              " 'Code quality',\n",
              " 'Code review',\n",
              " 'Compiler',\n",
              " 'Continuous integration',\n",
              " 'COVID-19',\n",
              " 'C++']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly we have defined functions for descriptions and URLs."
      ],
      "metadata": {
        "id": "P2k1s533BoqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* get_topic_descs function is used to get a information of topic Description"
      ],
      "metadata": {
        "id": "VfmAsYDscrk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_descs(doc):\n",
        "    desc_selector = 'f5 color-fg-muted mb-0 mt-1'\n",
        "    topic_desc_tags = doc.find_all('p', {'class': desc_selector})\n",
        "    topic_descs = []\n",
        "    for tag in topic_desc_tags:\n",
        "        topic_descs.append(tag.text.strip())\n",
        "    return topic_descs"
      ],
      "metadata": {
        "id": "RLneCOEiBi4S"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* topic_link_tag function is used to get a information of topic links"
      ],
      "metadata": {
        "id": "khIHcBN7drDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_urls(doc):\n",
        "    topic_link_tags = doc.find_all('a', {'class': 'no-underline flex-1 d-flex flex-column'})\n",
        "    topic_urls = []\n",
        "    base_url = 'https://github.com'\n",
        "    for tag in topic_link_tags:\n",
        "        topic_urls.append(base_url + tag['href'])\n",
        "    return topic_urls"
      ],
      "metadata": {
        "id": "bA2B8CGXBuGf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " -"
      ],
      "metadata": {
        "id": "sTj_xnD0B3sc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's put this all together into a single function"
      ],
      "metadata": {
        "id": "VD7zvmUaB3ZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* scrape_topics function is used to parse the information and store into topics_dict dictionary and covert in to data frame"
      ],
      "metadata": {
        "id": "lqKj_XYuegUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_topics():\n",
        "    topics_url = 'https://github.com/topics'\n",
        "    response = requests.get(topics_url)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception('Failed to load page {}'.format(topics_url))\n",
        "    doc = BeautifulSoup(response.text, 'html.parser')\n",
        "    topics_dict = {\n",
        "        'title': get_topic_titles(doc),\n",
        "        'description': get_topic_descs(doc),\n",
        "        'url': get_topic_urls(doc)\n",
        "    }\n",
        "    return pd.DataFrame(topics_dict)"
      ],
      "metadata": {
        "id": "1uXyiOFwBxAA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the top 25 repositories from a topic page"
      ],
      "metadata": {
        "id": "hXKnqZZ3CcAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* get_topic_page is used to download the topic url page and parse information using BeautifulSoup"
      ],
      "metadata": {
        "id": "cJ3aGGvwf6hw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_page(topic_url):\n",
        "    # Download the page\n",
        "    response = requests.get(topic_url)\n",
        "    # Check successful response\n",
        "    if response.status_code != 200:\n",
        "        raise Exception('Failed to load page {}'.format(topic_url))\n",
        "    # Parse using Beautiful soup\n",
        "    topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
        "    return topic_doc"
      ],
      "metadata": {
        "id": "2_fF0sSsCd9z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = get_topic_page('https://github.com/topics/3d')\n",
        "base_url = 'https://github.com'"
      ],
      "metadata": {
        "id": "v43PDFi3CeAy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* convert information 'k' to numbers"
      ],
      "metadata": {
        "id": "eSTVEvTTgim1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_star_count(stars):\n",
        "    stars=stars.strip()\n",
        "    if stars[-1]=='k':\n",
        "        return int(float(stars[:-1])*1000)\n",
        "    return(int(stars))"
      ],
      "metadata": {
        "id": "V3BIYvzlCeD3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* collect all information for repository and store in to variables"
      ],
      "metadata": {
        "id": "4Cai3Vr-g6_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_repo_info(h1_tag, star_tag):\n",
        "    # returns all the required info about a repository\n",
        "    a_tags = h1_tag.find_all('a')\n",
        "    username = a_tags[0].text.strip()\n",
        "    repo_name = a_tags[1].text.strip()\n",
        "    repo_url =  base_url + a_tags[1]['href']\n",
        "    stars = parse_star_count(star_tag.text.strip())\n",
        "    return username, repo_name, stars, repo_url"
      ],
      "metadata": {
        "id": "KNKlXYVpDGym"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* collect all the information and store in to dictionary and convert in to data frame"
      ],
      "metadata": {
        "id": "FOhpLmH4hO8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_repos(topic_doc):\n",
        "    # Get the h1 tags containing repo title, repo URL and username\n",
        "    repo_tags = topic_doc.find_all('article',{'class':'border rounded color-shadow-small color-bg-subtle my-4'})\n",
        "\n",
        "    # Get star tags\n",
        "    star_tags=topic_doc.find_all('span',{'id':'repo-stars-counter-star'})\n",
        "\n",
        "    topic_repos_dict = { 'username': [], 'repo_name': [], 'stars': [],'repo_url': []}\n",
        "\n",
        "    # Get repo info\n",
        "    for i in range(len(repo_tags)):\n",
        "        repo_info = get_repo_info(repo_tags[i], star_tags[i])\n",
        "        topic_repos_dict['username'].append(repo_info[0])\n",
        "        topic_repos_dict['repo_name'].append(repo_info[1])\n",
        "        topic_repos_dict['stars'].append(repo_info[2])\n",
        "        topic_repos_dict['repo_url'].append(repo_info[3])\n",
        "\n",
        "    return pd.DataFrame(topic_repos_dict)"
      ],
      "metadata": {
        "id": "4vLuOYLiDgu-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "create a CSV file"
      ],
      "metadata": {
        "id": "WVXR273ChiEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_topic(topic_url, path):\n",
        "    if os.path.exists(path):\n",
        "        print(\"The file {} already exists. Skipping...\".format(path))\n",
        "        return\n",
        "    topic_df = get_topic_repos(get_topic_page(topic_url))\n",
        "    topic_df.to_csv(path, index=None)"
      ],
      "metadata": {
        "id": "YW3WVInXDtYo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting it all together\n",
        "\n",
        "- We have a funciton to get the list of topics\n",
        "- We have a function to create a CSV file for scraped repos from a topics page\n",
        "- Let's create a function to put them together"
      ],
      "metadata": {
        "id": "3op8SPtdD4GZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* create all CSV file in topic page"
      ],
      "metadata": {
        "id": "saMEJS3xho9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_topics_repos():\n",
        "    print('Scraping list of topics')\n",
        "    topics_df = scrape_topics()\n",
        "\n",
        "    os.makedirs('data', exist_ok=True)\n",
        "    for index, row in topics_df.iterrows():\n",
        "        print('Scraping top repositories for \"{}\"'.format(row['title']))\n",
        "        scrape_topic(row['url'], 'data/{}.csv'.format(row['title']))"
      ],
      "metadata": {
        "id": "xpecQ-okDwIG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(scrape_topics_repos())"
      ],
      "metadata": {
        "id": "Ujmjijg0D664",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35de0487-9abe-474a-bb8b-622080564ef8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping list of topics\n",
            "Scraping top repositories for \"3D\"\n",
            "Scraping top repositories for \"Ajax\"\n",
            "Scraping top repositories for \"Algorithm\"\n",
            "Scraping top repositories for \"Amp\"\n",
            "Scraping top repositories for \"Android\"\n",
            "Scraping top repositories for \"Angular\"\n",
            "Scraping top repositories for \"Ansible\"\n",
            "Scraping top repositories for \"API\"\n",
            "Scraping top repositories for \"Arduino\"\n",
            "Scraping top repositories for \"ASP.NET\"\n",
            "Scraping top repositories for \"Atom\"\n",
            "Scraping top repositories for \"Awesome Lists\"\n",
            "Scraping top repositories for \"Amazon Web Services\"\n",
            "Scraping top repositories for \"Azure\"\n",
            "Scraping top repositories for \"Babel\"\n",
            "Scraping top repositories for \"Bash\"\n",
            "Scraping top repositories for \"Bitcoin\"\n",
            "Scraping top repositories for \"Bootstrap\"\n",
            "Scraping top repositories for \"Bot\"\n",
            "Scraping top repositories for \"C\"\n",
            "Scraping top repositories for \"Chrome\"\n",
            "Scraping top repositories for \"Chrome extension\"\n",
            "Scraping top repositories for \"Command line interface\"\n",
            "Scraping top repositories for \"Clojure\"\n",
            "Scraping top repositories for \"Code quality\"\n",
            "Scraping top repositories for \"Code review\"\n",
            "Scraping top repositories for \"Compiler\"\n",
            "Scraping top repositories for \"Continuous integration\"\n",
            "Scraping top repositories for \"COVID-19\"\n",
            "Scraping top repositories for \"C++\"\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read CSV files"
      ],
      "metadata": {
        "id": "C86Zwf5KE-Sp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/data/API.csv', index_col=None)\n",
        "print(df)"
      ],
      "metadata": {
        "id": "QN87duUVEAwL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5e47bc9-a3d1-45d1-970b-133571af655f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         username               repo_name   stars  \\\n",
            "0             NaN             public-apis  256000   \n",
            "1          neovim                  neovim   69400   \n",
            "2             NaN                tiangolo   62500   \n",
            "3             NaN                  strapi   56000   \n",
            "4             NaN              hoppscotch   54400   \n",
            "5             NaN                 ocornut   50300   \n",
            "6             NaN             meilisearch   38800   \n",
            "7       slatedocs                   slate   35400   \n",
            "8             NaN                    Kong   30700   \n",
            "9             NaN                  hasura   30100   \n",
            "10            NaN                  httpie   29200   \n",
            "11            NaN                    ccxt   29100   \n",
            "12      Binaryify    NeteaseCloudMusicApi   29100   \n",
            "13           trpc                    trpc   28700   \n",
            "14           YMFE                    yapi   26500   \n",
            "15         encode   django-rest-framework   26300   \n",
            "16  littlecodersh                  ItChat   24200   \n",
            "17            NaN                directus   23100   \n",
            "18      microsoft          api-guidelines   21800   \n",
            "19       shieldfy  API-Security-Checklist   21400   \n",
            "\n",
            "                                             repo_url  \n",
            "0                      https://github.com/public-apis  \n",
            "1                    https://github.com/neovim/neovim  \n",
            "2                         https://github.com/tiangolo  \n",
            "3                           https://github.com/strapi  \n",
            "4                       https://github.com/hoppscotch  \n",
            "5                          https://github.com/ocornut  \n",
            "6                      https://github.com/meilisearch  \n",
            "7                  https://github.com/slatedocs/slate  \n",
            "8                             https://github.com/Kong  \n",
            "9                           https://github.com/hasura  \n",
            "10                          https://github.com/httpie  \n",
            "11                            https://github.com/ccxt  \n",
            "12  https://github.com/Binaryify/NeteaseCloudMusicApi  \n",
            "13                       https://github.com/trpc/trpc  \n",
            "14                       https://github.com/YMFE/yapi  \n",
            "15    https://github.com/encode/django-rest-framework  \n",
            "16            https://github.com/littlecodersh/ItChat  \n",
            "17                        https://github.com/directus  \n",
            "18        https://github.com/microsoft/api-guidelines  \n",
            "19  https://github.com/shieldfy/API-Security-Check...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5U5X3T_gFXGj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}